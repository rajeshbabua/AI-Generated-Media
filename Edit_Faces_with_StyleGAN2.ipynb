{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Edit Faces with StyleGAN2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshbabua/AI-Generated-Media/blob/main/Edit_Faces_with_StyleGAN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7XFNv1wNboC"
      },
      "source": [
        "#Edit Faces with StyleGAN2\n",
        "###I've created this colab notebook upon popular request on my YT videos & on reddit. With it you can search for any person in the output space of StyleGAN2 (thx NVidia) and modify their age & gender & more with latent directions found by Robert Luxemburg ( https://twitter.com/robertluxemburg )\n",
        "\n",
        "#After reading this notebook, you will be able to produce videos similar to these:\n",
        "\n",
        "[![](http://img.youtube.com/vi/67wReX6C7VA/0.jpg)](http://www.youtube.com/watch?v=67wReX6C7VA \"Video Title\")\n",
        "[![](http://img.youtube.com/vi/Ty0PpXT0iE4/0.jpg)](http://www.youtube.com/watch?v=Ty0PpXT0iE4 \"Video Title\")\n",
        "\n",
        "###Read the instructions, modify the different options to your likings & run the cells!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i73mYHKPfys"
      },
      "source": [
        "Before running the setup cell, make sure you've selected a GPU runtime. Press runtime in the upper bar, click change runtime type & select GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lhKlNr3OQAv",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup (approximate time: 2 minutes)\n",
        "%tensorflow_version 1.x\n",
        "!git clone https://github.com/justinpinkney/stylegan2\n",
        "\n",
        "%cd stylegan2\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "\n",
        "import pretrained_networks\n",
        "\n",
        "def generate_images_in_w_space(dlatents, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    Gs_kwargs.truncation_psi = truncation_psi\n",
        "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        " \n",
        "    imgs = []\n",
        "    for row, dlatent in log_progress(enumerate(dlatents), name = \"Generating images\"):\n",
        "        #row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
        "        dl = (dlatent-dlatent_avg)*truncation_psi   + dlatent_avg\n",
        "        row_images = Gs.components.synthesis.run(dlatent,  **Gs_kwargs)\n",
        "        imgs.append(PIL.Image.fromarray(row_images[0], 'RGB'))\n",
        "    return imgs\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  a = np.asarray(a, dtype=np.uint8)\n",
        "  str_file = BytesIO()\n",
        "  PIL.Image.fromarray(a).save(str_file, format)\n",
        "  im_data = str_file.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print ('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format)\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def createImageGrid(images, scale=0.25, rows=1):\n",
        "   w,h = images[0].size\n",
        "   w = int(w*scale)\n",
        "   h = int(h*scale)\n",
        "   height = rows*h\n",
        "   cols = ceil(len(images) / rows)\n",
        "   width = cols*w\n",
        "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
        "   for i,img in enumerate(images):\n",
        "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
        "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
        "   return canvas\n",
        "\n",
        "def loadLatent(path):\n",
        "  return np.expand_dims(np.load(path),axis=0)\n",
        "\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n",
        "\n",
        "def interpolate(zs, steps):\n",
        "   out = []\n",
        "   for i in range(len(zs)-1):\n",
        "    for index in range(steps):\n",
        "     fraction = index/float(steps) \n",
        "     out.append(zs[i+1]*fraction + zs[i]*(1-fraction))\n",
        "   return out\n",
        "   \n",
        "import pretrained_networks\n",
        "\n",
        "network_pkl = \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"\n",
        " \n",
        "# If downloads fails, due to 'Google Drive download quota exceeded' you can try downloading manually from your own Google Drive account\n",
        "# network_pkl = \"/content/drive/My Drive/GAN/stylegan2-ffhq-config-f.pkl\"\n",
        " \n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "!mkdir raw\n",
        "!mkdir aligned\n",
        "!mkdir output\n",
        "\n",
        "print(\"Finished!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG5aPgk-PbGE"
      },
      "source": [
        "#Upload your images\n",
        "##Now upload your face image(s) to the raw folder in the stylegan2 directory. Press the folder icon on the left site to open the file browser if it isn't opened already.\n",
        "\n",
        "##The images should contain a human face. Good lighting conditions, high resolution & a central perspective will help to improve the results.\n",
        "\n",
        "###After you've done that, execute the next cell. It'll try to identify faces in the image(s) you uploaded & prepare them for the StyleGAN2 model & save the results in the aligned folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdPFV0jQpvg",
        "cellView": "form"
      },
      "source": [
        "#@title #Align Images. Download will sometimes get stuck, restarting likely helps. Alternatively, you can download & put the network in your drive & change the code in align_images to read the network from your drive.\n",
        "!python align_images.py raw/ aligned/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_EIdhn9ShIM"
      },
      "source": [
        "#Next step: Projecting your aligned images\n",
        "## In the following cell, you can adjust the parameter num-steps to your likings. A higher number will result in a longer execution time while it likely increases the quality of your output.\n",
        "### At higher step sizes weird artifcats may arise as a result of the network desperately trying to generate features it technically can't.\n",
        "\n",
        "\n",
        "#####Add \"--video True --video-mode 2\" to the arguments if you want to get a video of the projection process. It'll increase the runtime by a lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E9yz51-TLki"
      },
      "source": [
        "!python project_images.py --num-steps 350 aligned generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU-e7t1UUBy7"
      },
      "source": [
        "## After the previous cell has finished its execution you should find 2 files for each aligned image in stylegan2/generated.\n",
        "\n",
        "##One is a .png that shows how the generated person looked like after projection. If you want to reuse the projection you have just done you should download the .npy file. With it you can always load the latent (= input vector) with which you can generate the face that you can see in the .png.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf-yIc2OUvzy"
      },
      "source": [
        "npyPath = \"/content/stylegan2/generated/berniesanders_01.npy\" #Change this to fit the path of your file.\n",
        "latent = loadLatent(npyPath)\n",
        "imshow(generate_images_in_w_space([latent],1.0)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYgeRGIgWBTh"
      },
      "source": [
        "##If you have created multiple latents, you can interpolate between them.\n",
        "\n",
        "##Change steps to adjust the intermediate steps between two latents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K9MKCGWSJA"
      },
      "source": [
        "steps = 10\n",
        "fps = 25\n",
        "\n",
        "movieOutPath = \"interpolation.mp4\"\n",
        "\n",
        "latentA = loadLatent(\"/content/stylegan2/generated/elon musk_01.npy\") #Change path according to your case\n",
        "latentB = loadLatent(npyPath)\n",
        "\n",
        "latents = [latentA,latentB] #You can also add more latents here\n",
        "\n",
        "images = generate_images_in_w_space(interpolate(latents,steps),1.0)\n",
        "\n",
        "with imageio.get_writer(movieOutPath, mode='I', fps= fps) as writer:\n",
        "  for img in images:\n",
        "    writer.append_data(np.array(img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EItere6gXPAH"
      },
      "source": [
        "#To edit facial features like demonstrated in the videos of [this](https://www.youtube.com/playlist?list=PLDFtAMvjSK0Z88fSruugz8CZ4AqkZjDdZ) playlist, first download the directions.zip from [this](https://twitter.com/robertluxemburg/status/1207087801344372736) tweet. Upload them to the colab & unzip them using next the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SdXgG6VZO8x"
      },
      "source": [
        "#Adjust path if necessary\n",
        "!unzip /content/latent_directions.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4I-SsGIZZKf"
      },
      "source": [
        "latent = loadLatent(\"/content/stylegan2/generated/obama_01 (1).npy\") #Adjust this path to your case.\n",
        "feature = loadLatent(\"/content/stylegan2/stylegan2directions/smile.npy\") #Change the facial feature you want to edit here.\n",
        "\n",
        "steps = 9 \n",
        "#Negate the factor to go in the other direction\n",
        "factor = 2\n",
        "\n",
        "latents = []\n",
        "for i in range(steps):\n",
        "  latents.append(latent+feature*i*factor)\n",
        "\n",
        "imgs = generate_images_in_w_space(latents,1.0)\n",
        "\n",
        "import math\n",
        "imshow(createImageGrid(imgs, scale=0.2, rows=int(math.sqrt(steps))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP2ELd1EbL84"
      },
      "source": [
        "##LMK if you're missing an important feature in this notebook or if you need help.\n",
        "###Instead of creating an image grid you could also interpolate between the final edited latent & the original one. For that, simply copy & paste some code from 2 cells ago.\n",
        "\n",
        "##If you liked this notebook I'd be pleased if you expressed your gratitude by dropping a sub on my [YT channel](https://www.youtube.com/channel/UCIkA_Pi0VWSABdyAnmildpg?sub_confirmation=1). If you're generally interested in AI & new computer technologies, you are likely to enjoy the channel [Two Minute Papers](https://www.youtube.com/user/keeroyzw)."
      ]
    }
  ]
}